\documentclass{article}
\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{flafter}
\usepackage{chngcntr}
\counterwithin*{equation}{section}
\counterwithin*{equation}{subsection}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\title{ Machine Learning Methodologies and Applications (AI6012) Individual Assignment}
\author{Leon Sun (G2204908A)}
\date{29 September 2023}

\begin{document}
\maketitle
 

\section{Question 1 (10 marks)}

Multi-class classification, or Multinomial Logistic Regression, can be approached using softmax regression. The softmax function is generally defined as:

\begin{equation}
    \quad P(y=c \mid x)=\frac{\exp \left(w^{(c)} x\right)}{\sum_{i=1}^{C} \exp \left(w^{(i)} x\right)}\;\;or\;\;\frac{1}{1+\sum_{i\neq C}^{C} \exp \left(w^{i}\right).x}
\end{equation}
\linebreak

Since the sum of all the conditional probabilities for the softmax is 1, we can summarise the probabilities for all classes to:

\begin{equation}
    \sum_{c=0}^{C} \quad P(y=c \mid x)=1
\end{equation}

By introducing the set of logits into we can arrive at the following parametric equations for multinomial logistic regression. Suppose there are C classes, {0, 1, ..., C-1}:

\begin{equation}
    For\;c>0: \quad P(y=c \mid x)=\frac{\exp \left(-w^{(c)^T} x\right)}{1+\sum_{c=1}^{C-1} \exp \left(-w^{(c)^T} x\right)}=\hat{y}_c
\end{equation}

\begin{equation}
    For\;c>0: \quad P(y=0 \mid x)=\frac{1}{1+\sum_{c=1}^{C-1} \exp \left(-w^{(c)^T} x\right)}=\hat{y}_0
\end{equation}

% Maximum Likelihood Estimation (MLE)
Given a set of N training input-output pairs like {x\textsubscript{i}, y\textsubscript{i}}, i = 1,..., N, which are i.i.d, we can define the likehood as the product of likelihoods of each individual pairs.

\begin{equation}
    \mathcal{L}(\boldsymbol{w_c})=\prod_{i=1}^N l\left(\boldsymbol{w_c} \mid\left\{\boldsymbol{x}_i, y_i\right\}\right)=\prod_{i=1}^N P\left(y_i \mid \boldsymbol{x}_i ; \boldsymbol{w_c}\right)
    \end{equation}

Hence the maximum likelihood estimation can be represented in the following ln function which converts the product into a sum.
\begin{equation}
    \hat{\boldsymbol{w}_c}=\underset{\boldsymbol{w}_c}{\operatorname{argmax}} \prod_{i=1}^N P\left(y_i \mid \boldsymbol{x}_i ; \boldsymbol{w}_c\right)=\underset{\boldsymbol{w}_c}{\operatorname{argmax}} \sum_{i=1}^N \sum_{c=0}^{C-1}\left(y_i \ln \left(g\left(\boldsymbol{x}_i ; \boldsymbol{w}_c\right)\right)\right)
\end{equation}

\begin{equation}
    \ln \hat{w} =-\frac{1}{N} \sum_{i=1}^{N} \sum_{C=1}^{C-1} y_i\ln\left(\quad P(y_ \mid x_{j};w_{c})\right)
\end{equation}

We will now derive the learning procedure for the multinomial logistic classification using Gradient Descent optimisation method.

\begin{equation}
    \boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\rho \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol{w}}
\end{equation}


$$
\begin{aligned}
    \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol{w}}=\frac{\partial\left(-\sum_{i=1}^N \sum_{c=1}^c y_i \cdot \ln \left(P\left(y_i \mid x_i ; w_c\right)\right)\right)}{\partial w_c} \\
    = -\sum_{i=1}^N \sum_{c=1}^c \frac{\partial\left(y_i \cdot \ln \left(p\left(y_i \mid x_i ; w_c\right)\right)\right.}{\partial w_c}
\end{aligned}
$$

Let P(y\textsubscript{i} $\mid$ x\textsubscript{i}; w\textsubscript{c}) be f(z). Using chain rule:

\begin{equation}
    \begin{aligned}
        \frac{\partial \ln f(z)}{\partial z} & =\frac{\partial \ln f(z)}{\partial f(z)} \cdot \frac{\partial f(z)}{\partial z} \\
        & =\frac{1}{f(z)} \cdot \frac{d f(z)}{\partial z}
    \end{aligned}
\end{equation}

Applying the chain rule logic, we can obtain the derivative using: 

\begin{equation}
    \frac{\partial\left(y_i \cdot \ln \left(P\left(y_i \mid x_i ; w_c\right)\right)\right.}{\partial w_c}=y_i \cdot \frac{1}{P\left(y_i \mid x_{i} w_c\right)} \cdot \frac{\partial\left(P\left(y_i \mid x_i ; w_c\right)\right.}{\partial w_c}
\end{equation}
\linebreak

If y = c, subbing equations (3) into (10) gives us:

$$
\begin{aligned}
    \frac{\partial\left(P\left(y_i \mid x_i ; w_c\right)\right.}{\partial w_c} = \frac{\partial}{\partial w_c}\left(\frac{\exp \left(-w_c \cdot x_i \right)}{1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i \right)}\right)
\end{aligned}
$$
\linebreak

We will apply quotient rule to obtain the first order derivative:


\begin{align*}
    & \frac{\partial}{\partial w_c}\left(\frac{\exp \left(-w_c \cdot x_i \right)}{1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i \right)}\right) \\
    \\
    & =\frac{\left(\frac{\partial}{\partial w_c} \exp \left(-w_c \cdot x_i\right)\right)\left(1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i\right)\right)+\left(\frac{\partial}{\partial w_c}\left(1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i\right)\right)\right)\left(\exp \left(-w_c \cdot x_i\right)\right)}{\left(1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i\right)\right)^2} \\
    \\
    & =\frac{\left(x_i \cdot \exp \left(-w_c \cdot x_i\right)\right)\left(1+\sum_{i=1}^{c-1} \exp \left(-w_c \cdot x_i\right)\right)+x_i \cdot \exp \left(-w_c \cdot x_i\right) \cdot\left(\exp \left(-w_c \cdot x_i\right)\right)}{\left(1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i\right)\right)^2} \\ 
    \\
    & =\frac{x_i \exp \left(-W_c \cdot x_i\right)\left(1+\sum_{c=1}^{c-1} \exp \left(-W_c \cdot x_i\right)-\exp \left(-W_c \cdot x_i\right)\right)}{\left(1+\sum_{c=1}^{c-1} \exp \left(-W_c \cdot x_i\right)\right)^2} \\
    \\
    & =\frac{x_i \cdot \exp \left(-w_c \cdot x_i\right)}{1+\sum_{c=1}^{c-1} \operatorname{cop}\left(-w_c \cdot x_i\right)} \cdot \frac{1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i\right)-\exp \left(-w_c \cdot x_i\right)}{1+\sum_{c=1}^{c-1} \exp \left(-w_c \cdot x_i\right)} \\
    \\
    & =\frac{x_i \cdot \exp \left(-w_c \cdot x_i\right)}{1+\sum_{c=1}^{c-1} \exp \left(-w_c, x_i\right)} \cdot\left(\frac{1+\sum_{c=1}^{c-1} \exp \left(-w_c, x_i\right)}{1+\sum_{c=1}^{c-1} \exp \left(-w_c, x_i\right)}-\frac{\exp \left(-w_c \cdot x_i\right)}{1+\sum_{c=1}^{c-1} \exp \left(-w_{c i} x_i\right)}\right) \\
    \\
    & =x_i \cdot \hat{y}_c(1-\hat{y}_c) \numberthis \label{eqn}
\end{align*}

Subbing (11) back into (10):

\begin{align*}
    & \frac{\partial\left(P\left(y_i \mid x_i ; w_c\right)\right.}{\partial w_c} \\
    & = y_i \cdot \frac{1}{P\left(y_i \mid x_{i} w_c\right)} \cdot \frac{\partial\left(P\left(y_i \mid x_i ; w_c\right)\right.}{\partial w_c} \\
    & = y_i \cdot \frac{1}{\hat{y}_c} \cdot (x_i \cdot \hat{y}_c(1-\hat{y}_c)) \\
    & = x_i(y_i-y_i \cdot \hat{y}_c) \numberthis \label{eqn}
\end{align*}

Putting them back together, we will sub (12) into the gradient descent rule (8)

\begin{align*}
    & \boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\rho \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol{w}} \\
    & =  \boldsymbol{w}_t-\rho \left(-\sum_{i=1}^N \sum_{c=1}^C \frac{\partial\left(y_i \cdot \ln \left(p\left(y_i \mid x_i ; w_c\right)\right)\right.}{\partial w_c} - \lambda \boldsymbol{w}\right) \\
    & = \boldsymbol{w}_t+\rho \left(\sum_{i=1}^N \left(y_i-y_i \cdot \hat{y}_c\right)x_i - \lambda \boldsymbol{w}\right) \numberthis \label{eqn}
\end{align*}


\section{Question 2 (5 marks)}

\subsection*{2.2. Answer:}
% Q2.2 LinearSVM table
\begin{table}[!hbt]
    \centering
    % \resizebox{\columnwidth}{!}{%
    \resizebox{0.5\columnwidth}{!}{%
        \begin{tabular}{lllll}
            \hline
            \multicolumn{1}{|c|}{\textbf{C=0.01}} & \multicolumn{1}{c|}{\textbf{C=0.05}} & \multicolumn{1}{c|}{\textbf{C=0.1}} & \multicolumn{1}{c|}{\textbf{C=0.5}} & \multicolumn{1}{c|}{\textbf{C=1}} \\ \hline
            \multicolumn{1}{|c|}{0.84402} & \multicolumn{1}{c|}{0.84610} & \multicolumn{1}{c|}{ 0.84644} & \multicolumn{1}{c|}{0.84693} & \multicolumn{1}{c|}{{\color[HTML]{FE0000}0.84721}} \\ \hline
        \end{tabular}%
    }    
    \caption{Classification accuracy on running linear kernel SVM on 3-fold cross-validation using training set with different values of the parameter C in \{0.01, 0.05, 0.1, 0.5, 1\}}
    \label{tab:linearSVM}
\end{table}

\subsection*{2.3. Answer:}
\begin{table}[!hbt]
    \centering
    \resizebox{0.8\columnwidth}{!}{%
        \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \multicolumn{1}{|c|}{} & \textbf{g=0.01} & \textbf{g=0.05} & \textbf{g=0.1} & \textbf{g=0.5} & \textbf{g=1} \\ \hline
        \textbf{C=0.01} & 0.75919 & 0.81991 & 0.81985 & 0.75919 & 0.75919 \\ \hline
        \textbf{C=0.05} & 0.83121 & 0.83575 & 0.83425 & 0.78916 & 0.75919 \\ \hline
        \textbf{C=0.1} & 0.83772 & 0.83965 & 0.83876 & 0.80612 & 0.76199 \\ \hline
        \textbf{C=0.5} & 0.84297 & 0.84577 & 0.84681 & 0.83216 & 0.78975 \\ \hline
        \textbf{C=1} & 0.84442 & 0.84675 & {\color[HTML]{FE0000} 0.84742} & 0.83661 & 0.79829 \\ \hline
        \end{tabular}%
    }
    \caption{Classification accuracy on running rbf kernel SVM on 3-fold cross-validation using training set with parameter gamma in \{0.01, 0.05, 0.1, 0.5, 1\} and different values of the parameter C in \{0.01, 0.05, 0.1, 0.5, 1\}}
    \label{tab:rbfSVM}
\end{table}

\subsection*{2.4. Answer:}

\begin{table}[!htb]
    \centering
    \begin{tabular}{ll}
        \hline
        \multicolumn{1}{|c|}{\textbf{}} & \multicolumn{1}{c|}{\textbf{kernel=RBF, C=1, gamma=0.1}} \\ \hline
        \multicolumn{1}{|c|}{\textbf{Accuracy of SVMs}} & \multicolumn{1}{c|}{0.84614} \\ \hline
    \end{tabular}
    \caption{Classification accuracy on running rbf kernel SVM on 3-fold cross-validation using test set with C=1 and gamma=0.1}
    \label{tab:OptimalSVM}
\end{table}

\section{Question 3 (5 marks)}

Linear soft margin SVMs:
\begin{equation}
    \min _{\boldsymbol{w}, b, \xi_i} \frac{\|\boldsymbol{w}\|_2^2}{2}+C\left(\sum_{i=1}^N \xi_i\right)
\end{equation}

Empirical Structural Risk Minimisation

\begin{equation}
    \widehat{\boldsymbol{\theta}}=\arg \min _{\boldsymbol{\theta}} \sum_{i=1}^N \ell\left(f\left(\boldsymbol{x}_i ; \boldsymbol{\theta}\right), y_i\right)+\lambda \Omega(\boldsymbol{\theta})
\end{equation}

To reformulate the optimisation of linear non SVMs as an instance of empirical structural risk minimisation, we will leverage on hinge loss. \\

\begin{center}
    if $\left(w \cdot x_i+b\right)_{y_i} \geqslant 1$, 
    \begin{equation}
        \varepsilon_i^*(w, b)=0
    \end{equation}
    if $(w, x ;+b) y_i<1$,
    \begin{align*}
        & \varepsilon_i^*(w, b)=1-\left(w, x_i+b\right) y_i \numberthis \label{eqn} \\  
        & \therefore \varepsilon_i=\max \left(0,1-\left(w_i x_i+b\right)_{y_i}\right) \numberthis \label{eqn}
    \end{align*} 
    
\end{center}

We can then derive the empirical structural risk minimisation:
\begin{equation}
    \sum_{i=1}^N \varepsilon_i= \sum_{i=1}^N \max \left(0, 1-\left(w_i \cdot x_i+b\right)_{y_i}\right)
\end{equation}

Substituting them back into the objective will give us the hinge loss reformulation of the linear non SVM.

\begin{equation}
    \min _{w, b}\|w\|_2^2+C \sum_{i=1}^N \max \left(0,1-\left(w_i x_i+b\right) y_i\right)
\end{equation}

\section{Question 4 (5 marks)}

The regularised linear regression can be represented as an optimisation problem given by this formula:

\begin{align*}
    \hat{w}=\arg\operatorname*{min}_{w}\frac{1}{2}\sum_{i=1}^{N}(w\cdot x_{i}-y_{i})^{2}+\frac{\lambda}{2}\|w\|_{2}^{2} \numberthis \label{eqn}
\end{align*} 

Using the kernel trick, we can map the current dimensional space into a feature space to extend this regularised linear regression for solving non linear problems.

\begin{align*}
    \hat{w}=\arg\operatorname*{min}_{w}\frac{1}{2}~\sum_{i=1}^{N}~~(w \cdot \Psi(x_{i})-y_{i})^2 + \frac{\lambda}{2}\|w\|_{2}^{2} \numberthis \label{eqn}
\end{align*}

To obtain a closed form solution, the derivative of the objective w.r.t. w will be set to 0. Solving the resultant equation:

\begin{align*}
    & \frac{\partial\left(\frac{1}{2}\sum_{i=1}^{N}(w\cdot \psi(x_{i})-y_{i})^{2}+\frac{\lambda}{2}\left|w\right|_{2}^{2}\right)}{\partial w}=0 \\
    & \frac{\partial\left(\frac{1}{2}\sum_{i=1}^{N}(w\cdot \psi(x_{i})-y_{i})^{2}\right)}{\partial w}+ {\frac{\partial \left(\frac{\lambda}{2}\left|w\right|_{2}^{2}\right)}{\partial w}} = 0 \\
    & \left(\sum_{i=1}^{N}\left(\psi(x_{i})\right) \left(\psi(x_{i}^{T})\right)\right)w-\sum_{i=1}^{N}y_{i}\left(\psi (x_{i}) \right) + \lambda w=0 \numberthis \label{eqn}
\end{align*}

Let $\sum_{i=1}^{N}y_{i}\left(\psi (x_{i}) \right) = K$, where K represents the sum of inner product between mapped instances. Subbing this back to (3) gives us:

\begin{align*}
    &\mathbf{KK^Tw}-\mathbf{Ky}+ \mathbf{\lambda Iw}=0 \\
    & (\mathbf{KK^T}+\mathbf{\lambda I})\mathbf{w} - \mathbf{Ky}=0 \\
    & (\mathbf{KK^T}+\mathbf{\lambda I})\mathbf{w} = \mathbf{Ky} \\
    & \mathbf{w}=(\mathbf{KK^T}+\mathbf{\lambda I})^{-1}\mathbf{Ky} \numberthis \label{eqn}
\end{align*}


\end{document}
